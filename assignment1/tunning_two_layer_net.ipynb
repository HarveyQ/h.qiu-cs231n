{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Two Layer Neural Network\n",
    "# hyperparameter tunning\n",
    "This new notebook is to:\n",
    "1. clean up the cs231n official note\n",
    "2. isolate the tunning part of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# send wechat message at the end of training\n",
    "import itchat\n",
    "\n",
    "itchat.auto_login(enableCmdQR=-2)\n",
    "train_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from cs231n.classifiers.neural_net import TwoLayerNet\n",
    "from cs231n.classifiers.neural_net_tunning import hyper_params_comb, net_tunning\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# some of the functions that might come in handy later\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "\n",
    "# load CIFAR-10 data\n",
    "from cs231n.data_utils import load_CIFAR10, get_CIFAR10_data\n",
    "\n",
    "data_set = get_CIFAR10_data()\n",
    "\n",
    "\n",
    "# unpack the data\n",
    "# re-run this to reload X_train and y_train\n",
    "X_train = data_set['X_train']\n",
    "y_train = data_set['y_train']\n",
    "X_val = data_set['X_val']\n",
    "y_val = data_set['y_val']\n",
    "X_test = data_set['X_test']\n",
    "y_test = data_set['y_test']\n",
    "\n",
    "# check dimensions\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the developing dataset\n",
    "num_dev = 500\n",
    "mask_dev = np.random.choice(X_train.shape[0], num_dev, replace=False)\n",
    "X_dev = X_train[mask_dev]\n",
    "y_dev = y_train[mask_dev]\n",
    "print('Dev data shape: ', X_dev.shape)\n",
    "print('Dev labels shape: ', y_dev.shape)\n",
    "\n",
    "# use developing data if dev_mode is on\n",
    "dev_mode = False  # developing mode flag: True(on) or False(off)\n",
    "if dev_mode:\n",
    "    X_train = X_dev\n",
    "    y_train = y_dev\n",
    "    print('Notice: using dev dataset!')\n",
    "else:\n",
    "    del X_dev, y_dev\n",
    "    print('Caution: using full training dataset!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that all list/array of hyperparameters are arranged in the order of:  \n",
    "[hidden_size, learning_rate, num_epochs, reg]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Computation speed comparison: gcloud vs macbook\n",
    "The quotas on this gcloud VM instance is:\n",
    "- 8 vCPU(s): Intel Xeon @ 2.2 GHz\n",
    "- RAM: 30GB\n",
    "- ROM: 10GB persistent disk\n",
    "\n",
    "On Macbook:  \n",
    "- 1 CPU: Intel(R) Core(TM) i5-5250U CPU @ 1.60GHz (boost 2.7GHz)\n",
    "- 4GB 1600MHz DDR3\n",
    "\n",
    "The following section tests actual running time of training 5 epochs, averaged over 5 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hidden_size_range = [204]\n",
    "# learning_rate_range = [1e-3]\n",
    "# num_epochs_range = [7]\n",
    "# reg_range = [1.20e-06]\n",
    "\n",
    "# hyper_params_range = [hidden_size_range, learning_rate_range, num_epochs_range, reg_range]\n",
    "# hyper_params_list = hyper_params_comb(hyper_params_range)\n",
    "\n",
    "# tic = time.time()\n",
    "# best_net, results = net_tunning(X_train, y_train, \n",
    "#                                X_val, y_val, \n",
    "#                                hyper_params_list, verbose=False)\n",
    "# toc = time.time()\n",
    "\n",
    "# # time\n",
    "# total_time = float(toc-tic)\n",
    "# avg_time = total_time / len(hyper_params_list)\n",
    "# print('Time elapsed: %f' % (toc-tic))\n",
    "# print('Average per training: %f' % (avg_time))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results (train + val time per 5 epochs):  \n",
    "- on macbook: 94.24 seconds\n",
    "- on gcloud VM: 31.35 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Hidden size tunning:**  \n",
    "Coarse tunning:  \n",
    "- tried [100~300] for 10 evenly spaced numbers\n",
    "- best validation accuracy: 0.513000 @ H = 210\n",
    "- best training accuracy: 0.533612\n",
    "\n",
    "Fine tunning:  \n",
    "- try [200 - 220] every one\n",
    "- best validation accuracy:   @ H = 204\n",
    "- best training accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 1.1 hidden size: coarse search\n",
    "# # set range of tunning for hyperparameters\n",
    "# hidden_size_range = np.arange(201, 221, 1)\n",
    "# learning_rate_range = [1e-3]\n",
    "# num_epochs_range = [5]\n",
    "# reg_range = [0.25]\n",
    "# # drop = 1  # single on/off flag\n",
    "# # drop_prob = 0.5\n",
    "\n",
    "\n",
    "# hyper_params_range = [hidden_size_range, \n",
    "#                       learning_rate_range, \n",
    "#                       num_epochs_range, \n",
    "#                       reg_range]\n",
    "\n",
    "# # net model tunning\n",
    "# hyper_params_list = hyper_params_comb(hyper_params_range)\n",
    "# print('Number of hyperparms to tune: %d' % len(hyper_params_list))\n",
    "# tic = time.time()\n",
    "# best_net, results = net_tunning(X_train, y_train, \n",
    "#                                X_val, y_val, \n",
    "#                                hyper_params_list, verbose=False)\n",
    "# toc = time.time()\n",
    "\n",
    "# # time\n",
    "# print()\n",
    "# print('Number of hyperparams to tune: %d' % len(hyper_params_list))\n",
    "# print('Total time used: %f (seconds)' % (toc-tic))\n",
    "# print('Total time per hyperparam: %f (seconds)' % (float(toc-tic)/float(len(hyper_params_list))))\n",
    "# print()\n",
    "\n",
    "\n",
    "# # visualise of results: hidden_size vs train/val accuracy\n",
    "# hidden_size_axis = [hyper_params[0] for hyper_params in sorted(results)]\n",
    "# train_acc_history = [results[hyper_params][0] for hyper_params in sorted(results)]\n",
    "# val_acc_history = [results[hyper_params][1] for hyper_params in sorted(results)]\n",
    "\n",
    "# plt.plot(hidden_size_axis, val_acc_history, label='val')\n",
    "# plt.plot(hidden_size_axis, train_acc_history, label='train')\n",
    "# plt.title('Coarse tunning of hidden size')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Regluarisation running:\n",
    "Notice: fixed hidden size = 210\n",
    "Coarse tunning:  \n",
    "- 1st tunning: 10^[-2~2] for 20 evenly spaced number\n",
    "    - results: best validation accuracy 0.498 @ reg = 0.042813\n",
    "    - best results at the lower edge\n",
    "- 2nd tunning: 10^[-5~1] for 20 evenly spaced numbers    \n",
    "    - results: best validation accuracy 0.521000 @ **reg = 0.003360**\n",
    "    \n",
    "Fine tunning:\n",
    "- 10^[-6 ~ -5] random searching 20 numbers (uniform distribution)\n",
    "    - results: best validation accuracy 0.512 @ **reg = 1.20e-06**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 2.1 regularisation: fine search\n",
    "# # coarse search of reg with hidden_size = 204 (best from above)\n",
    "# # set range of tunning for hyperparameters\n",
    "# # reg_range = 10 ** np.linspace(-5, 1, 20)  # 1st coarse tunning\n",
    "# # reg_range = 10 ** (np.random.uniform(-6, -5, 20)) # 2nd coarse tunning\n",
    "# reg_range = 10 ** (np.random.uniform(-6, -5.8, 10))  # 1st fine tunning\n",
    "\n",
    "# hidden_size_range = [204]\n",
    "# learning_rate_range = [1e-3]\n",
    "# num_epochs_range = [5]\n",
    "# # drop = 1  # single on/off flag\n",
    "# # drop_prob = 0.5\n",
    "\n",
    "\n",
    "# hyper_params_range = [hidden_size_range, \n",
    "#                       learning_rate_range, \n",
    "#                       num_epochs_range, \n",
    "#                       reg_range]\n",
    "\n",
    "# # net model tunning\n",
    "# hyper_params_list = hyper_params_comb(hyper_params_range)\n",
    "# tic = time.time()\n",
    "# best_net, results = net_tunning(X_train, y_train, \n",
    "#                                X_val, y_val, \n",
    "#                                hyper_params_list, verbose=False)\n",
    "# toc = time.time()\n",
    "\n",
    "# # time\n",
    "# print()\n",
    "# print('Number of hyperparams to tune: %d' % len(hyper_params_list))\n",
    "# print('Total time used: %f (seconds)' % (toc-tic))\n",
    "# print('Total time per hyperparam: %f (seconds)' % (float(toc-tic)/float(len(hyper_params_list))))\n",
    "# print()\n",
    "\n",
    "\n",
    "# # visualise of results: hidden_size vs train/val accuracy\n",
    "# reg_axis = [hyper_params[3] for hyper_params in sorted(results)]\n",
    "# train_acc_history = [results[hyper_params][0] for hyper_params in sorted(results)]\n",
    "# val_acc_history = [results[hyper_params][1] for hyper_params in sorted(results)]\n",
    "\n",
    "# plt.subplot(2,1,1)\n",
    "# plt.plot(reg_axis, val_acc_history, label='val')\n",
    "# plt.plot(reg_axis, train_acc_history, label='train')\n",
    "# plt.title('Coarse tunning of regularisation strength')\n",
    "# plt.legend()\n",
    "             \n",
    "# plt.subplot(2,1,2)\n",
    "# plt.plot(np.log10(reg_axis), val_acc_history, label='val')\n",
    "# plt.plot(np.log10(reg_axis), train_acc_history, label='train')\n",
    "# plt.title('Coarse tunning of regularisation strength, logirithm axis')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10 / 1225: loss 2.300941\n",
      "W2 norm: 0.000035\n",
      "W1 norm: 0.006286\n",
      "iteration 20 / 1225: loss 2.286067\n",
      "W2 norm: 0.000173\n",
      "W1 norm: 0.006422\n",
      "iteration 30 / 1225: loss 2.195848\n",
      "W2 norm: 0.000977\n",
      "W1 norm: 0.007222\n",
      "iteration 40 / 1225: loss 2.113286\n",
      "W2 norm: 0.002412\n",
      "W1 norm: 0.008644\n",
      "iteration 50 / 1225: loss 2.057474\n",
      "W2 norm: 0.004035\n",
      "W1 norm: 0.010253\n",
      "iteration 60 / 1225: loss 2.048025\n",
      "W2 norm: 0.005432\n",
      "W1 norm: 0.011657\n",
      "iteration 70 / 1225: loss 2.034057\n",
      "W2 norm: 0.007150\n",
      "W1 norm: 0.013387\n",
      "iteration 80 / 1225: loss 2.003117\n",
      "W2 norm: 0.008834\n",
      "W1 norm: 0.015099\n",
      "iteration 90 / 1225: loss 1.958606\n",
      "W2 norm: 0.010357\n",
      "W1 norm: 0.016739\n",
      "iteration 100 / 1225: loss 1.901248\n",
      "W2 norm: 0.011991\n",
      "W1 norm: 0.018480\n",
      "iteration 110 / 1225: loss 1.887584\n",
      "W2 norm: 0.013908\n",
      "W1 norm: 0.020572\n",
      "iteration 120 / 1225: loss 1.890167\n",
      "W2 norm: 0.015330\n",
      "W1 norm: 0.022170\n",
      "iteration 130 / 1225: loss 1.837199\n",
      "W2 norm: 0.017206\n",
      "W1 norm: 0.024204\n",
      "iteration 140 / 1225: loss 1.734436\n",
      "W2 norm: 0.018640\n",
      "W1 norm: 0.025857\n",
      "iteration 150 / 1225: loss 1.774877\n",
      "W2 norm: 0.020146\n",
      "W1 norm: 0.027659\n",
      "iteration 160 / 1225: loss 1.731238\n",
      "W2 norm: 0.021460\n",
      "W1 norm: 0.029304\n",
      "iteration 170 / 1225: loss 1.779576\n",
      "W2 norm: 0.022607\n",
      "W1 norm: 0.030790\n",
      "iteration 180 / 1225: loss 1.661873\n",
      "W2 norm: 0.024559\n",
      "W1 norm: 0.033059\n",
      "iteration 190 / 1225: loss 1.739470\n",
      "W2 norm: 0.025978\n",
      "W1 norm: 0.034812\n",
      "iteration 200 / 1225: loss 1.756111\n",
      "W2 norm: 0.027182\n",
      "W1 norm: 0.036438\n",
      "iteration 210 / 1225: loss 1.739520\n",
      "W2 norm: 0.028414\n",
      "W1 norm: 0.038089\n",
      "iteration 220 / 1225: loss 1.680335\n",
      "W2 norm: 0.030376\n",
      "W1 norm: 0.040475\n",
      "iteration 230 / 1225: loss 1.741881\n",
      "W2 norm: 0.031501\n",
      "W1 norm: 0.041972\n",
      "iteration 240 / 1225: loss 1.660501\n",
      "W2 norm: 0.033266\n",
      "W1 norm: 0.044191\n",
      "number of epochs completed: 1\n",
      "iteration 250 / 1225: loss 1.607963\n",
      "W2 norm: 0.035479\n",
      "W1 norm: 0.046923\n",
      "iteration 260 / 1225: loss 1.669214\n",
      "W2 norm: 0.036742\n",
      "W1 norm: 0.048724\n",
      "iteration 270 / 1225: loss 1.696249\n",
      "W2 norm: 0.038089\n",
      "W1 norm: 0.050626\n",
      "iteration 280 / 1225: loss 1.624195\n",
      "W2 norm: 0.039709\n",
      "W1 norm: 0.052732\n",
      "iteration 290 / 1225: loss 1.768447\n",
      "W2 norm: 0.040649\n",
      "W1 norm: 0.054294\n",
      "iteration 300 / 1225: loss 1.598089\n",
      "W2 norm: 0.041978\n",
      "W1 norm: 0.056361\n",
      "iteration 310 / 1225: loss 1.620830\n",
      "W2 norm: 0.043363\n",
      "W1 norm: 0.058353\n",
      "iteration 320 / 1225: loss 1.562676\n",
      "W2 norm: 0.045226\n",
      "W1 norm: 0.060822\n",
      "iteration 330 / 1225: loss 1.599063\n",
      "W2 norm: 0.046865\n",
      "W1 norm: 0.063083\n",
      "iteration 340 / 1225: loss 1.718276\n",
      "W2 norm: 0.047923\n",
      "W1 norm: 0.064923\n",
      "iteration 350 / 1225: loss 1.659446\n",
      "W2 norm: 0.049371\n",
      "W1 norm: 0.067212\n",
      "iteration 360 / 1225: loss 1.592998\n",
      "W2 norm: 0.050349\n",
      "W1 norm: 0.069019\n",
      "iteration 370 / 1225: loss 1.522494\n",
      "W2 norm: 0.052175\n",
      "W1 norm: 0.071584\n",
      "iteration 380 / 1225: loss 1.647505\n",
      "W2 norm: 0.053324\n",
      "W1 norm: 0.073609\n",
      "iteration 390 / 1225: loss 1.583601\n",
      "W2 norm: 0.054814\n",
      "W1 norm: 0.075915\n",
      "iteration 400 / 1225: loss 1.598252\n",
      "W2 norm: 0.055827\n",
      "W1 norm: 0.077713\n",
      "iteration 410 / 1225: loss 1.442617\n",
      "W2 norm: 0.057244\n",
      "W1 norm: 0.080054\n",
      "iteration 420 / 1225: loss 1.567434\n",
      "W2 norm: 0.058437\n",
      "W1 norm: 0.082209\n",
      "iteration 430 / 1225: loss 1.614309\n",
      "W2 norm: 0.059893\n",
      "W1 norm: 0.084530\n",
      "iteration 440 / 1225: loss 1.674737\n",
      "W2 norm: 0.061447\n",
      "W1 norm: 0.086958\n",
      "iteration 450 / 1225: loss 1.550848\n",
      "W2 norm: 0.062467\n",
      "W1 norm: 0.088895\n",
      "iteration 460 / 1225: loss 1.461834\n",
      "W2 norm: 0.063603\n",
      "W1 norm: 0.091092\n",
      "iteration 470 / 1225: loss 1.633876\n",
      "W2 norm: 0.064452\n",
      "W1 norm: 0.092995\n",
      "iteration 480 / 1225: loss 1.578965\n",
      "W2 norm: 0.066152\n",
      "W1 norm: 0.095660\n",
      "iteration 490 / 1225: loss 1.530157\n",
      "W2 norm: 0.067248\n",
      "W1 norm: 0.097675\n",
      "number of epochs completed: 2\n",
      "iteration 500 / 1225: loss 1.625365\n",
      "W2 norm: 0.068413\n",
      "W1 norm: 0.099716\n",
      "iteration 510 / 1225: loss 1.510865\n",
      "W2 norm: 0.069858\n",
      "W1 norm: 0.102173\n",
      "iteration 520 / 1225: loss 1.450292\n",
      "W2 norm: 0.070943\n",
      "W1 norm: 0.104148\n",
      "iteration 530 / 1225: loss 1.664558\n",
      "W2 norm: 0.071696\n",
      "W1 norm: 0.106162\n",
      "iteration 540 / 1225: loss 1.678117\n",
      "W2 norm: 0.072737\n",
      "W1 norm: 0.108222\n",
      "iteration 550 / 1225: loss 1.616526\n",
      "W2 norm: 0.073938\n",
      "W1 norm: 0.110518\n",
      "iteration 560 / 1225: loss 1.525638\n",
      "W2 norm: 0.075279\n",
      "W1 norm: 0.112910\n",
      "iteration 570 / 1225: loss 1.553580\n",
      "W2 norm: 0.076737\n",
      "W1 norm: 0.115385\n",
      "iteration 580 / 1225: loss 1.439728\n",
      "W2 norm: 0.078156\n",
      "W1 norm: 0.117959\n",
      "iteration 590 / 1225: loss 1.514789\n",
      "W2 norm: 0.079757\n",
      "W1 norm: 0.120568\n",
      "iteration 600 / 1225: loss 1.605686\n",
      "W2 norm: 0.080419\n",
      "W1 norm: 0.122260\n",
      "iteration 610 / 1225: loss 1.465879\n",
      "W2 norm: 0.081489\n",
      "W1 norm: 0.124525\n",
      "iteration 620 / 1225: loss 1.492422\n",
      "W2 norm: 0.082275\n",
      "W1 norm: 0.126442\n",
      "iteration 630 / 1225: loss 1.454916\n",
      "W2 norm: 0.083338\n",
      "W1 norm: 0.128609\n",
      "iteration 640 / 1225: loss 1.452727\n",
      "W2 norm: 0.084677\n",
      "W1 norm: 0.131124\n",
      "iteration 650 / 1225: loss 1.349165\n",
      "W2 norm: 0.086539\n",
      "W1 norm: 0.134136\n",
      "iteration 660 / 1225: loss 1.552736\n",
      "W2 norm: 0.087182\n",
      "W1 norm: 0.135909\n",
      "iteration 670 / 1225: loss 1.406838\n",
      "W2 norm: 0.088884\n",
      "W1 norm: 0.138767\n",
      "iteration 680 / 1225: loss 1.414318\n",
      "W2 norm: 0.090805\n",
      "W1 norm: 0.141745\n",
      "iteration 690 / 1225: loss 1.557815\n",
      "W2 norm: 0.091488\n",
      "W1 norm: 0.143722\n",
      "iteration 700 / 1225: loss 1.489129\n",
      "W2 norm: 0.092237\n",
      "W1 norm: 0.145783\n",
      "iteration 710 / 1225: loss 1.455931\n",
      "W2 norm: 0.093166\n",
      "W1 norm: 0.148031\n",
      "iteration 720 / 1225: loss 1.472461\n",
      "W2 norm: 0.094579\n",
      "W1 norm: 0.150621\n",
      "iteration 730 / 1225: loss 1.473171\n",
      "W2 norm: 0.095301\n",
      "W1 norm: 0.152712\n",
      "number of epochs completed: 3\n",
      "iteration 740 / 1225: loss 1.420345\n",
      "W2 norm: 0.096247\n",
      "W1 norm: 0.155183\n",
      "iteration 750 / 1225: loss 1.450178\n",
      "W2 norm: 0.097364\n",
      "W1 norm: 0.157460\n",
      "iteration 760 / 1225: loss 1.467869\n",
      "W2 norm: 0.097980\n",
      "W1 norm: 0.159318\n",
      "iteration 770 / 1225: loss 1.342875\n",
      "W2 norm: 0.099193\n",
      "W1 norm: 0.161716\n",
      "iteration 780 / 1225: loss 1.468794\n",
      "W2 norm: 0.100073\n",
      "W1 norm: 0.163834\n",
      "iteration 790 / 1225: loss 1.313878\n",
      "W2 norm: 0.101548\n",
      "W1 norm: 0.166428\n",
      "iteration 800 / 1225: loss 1.479908\n",
      "W2 norm: 0.102665\n",
      "W1 norm: 0.168846\n",
      "iteration 810 / 1225: loss 1.519739\n",
      "W2 norm: 0.103536\n",
      "W1 norm: 0.170931\n",
      "iteration 820 / 1225: loss 1.451856\n",
      "W2 norm: 0.104160\n",
      "W1 norm: 0.173072\n",
      "iteration 830 / 1225: loss 1.496572\n",
      "W2 norm: 0.104705\n",
      "W1 norm: 0.175028\n",
      "iteration 840 / 1225: loss 1.464571\n",
      "W2 norm: 0.106469\n",
      "W1 norm: 0.178173\n",
      "iteration 850 / 1225: loss 1.378839\n",
      "W2 norm: 0.107501\n",
      "W1 norm: 0.180733\n",
      "iteration 860 / 1225: loss 1.389644\n",
      "W2 norm: 0.108673\n",
      "W1 norm: 0.183479\n",
      "iteration 870 / 1225: loss 1.372090\n",
      "W2 norm: 0.110016\n",
      "W1 norm: 0.186329\n",
      "iteration 880 / 1225: loss 1.503576\n",
      "W2 norm: 0.110830\n",
      "W1 norm: 0.188626\n",
      "iteration 890 / 1225: loss 1.304366\n",
      "W2 norm: 0.111891\n",
      "W1 norm: 0.190987\n",
      "iteration 900 / 1225: loss 1.534939\n",
      "W2 norm: 0.113134\n",
      "W1 norm: 0.193524\n",
      "iteration 910 / 1225: loss 1.412177\n",
      "W2 norm: 0.114291\n",
      "W1 norm: 0.196153\n",
      "iteration 920 / 1225: loss 1.425856\n",
      "W2 norm: 0.114715\n",
      "W1 norm: 0.198381\n",
      "iteration 930 / 1225: loss 1.342615\n",
      "W2 norm: 0.115820\n",
      "W1 norm: 0.200778\n",
      "iteration 940 / 1225: loss 1.236727\n",
      "W2 norm: 0.116871\n",
      "W1 norm: 0.203055\n",
      "iteration 950 / 1225: loss 1.332651\n",
      "W2 norm: 0.118098\n",
      "W1 norm: 0.205661\n",
      "iteration 960 / 1225: loss 1.483949\n",
      "W2 norm: 0.118388\n",
      "W1 norm: 0.207474\n",
      "iteration 970 / 1225: loss 1.365468\n",
      "W2 norm: 0.118988\n",
      "W1 norm: 0.209575\n",
      "iteration 980 / 1225: loss 1.401844\n",
      "W2 norm: 0.120369\n",
      "W1 norm: 0.212443\n",
      "number of epochs completed: 4\n",
      "iteration 990 / 1225: loss 1.403269\n",
      "W2 norm: 0.121151\n",
      "W1 norm: 0.214760\n",
      "iteration 1000 / 1225: loss 1.355788\n",
      "W2 norm: 0.122096\n",
      "W1 norm: 0.217029\n",
      "iteration 1010 / 1225: loss 1.276656\n",
      "W2 norm: 0.123513\n",
      "W1 norm: 0.219614\n",
      "iteration 1020 / 1225: loss 1.349443\n",
      "W2 norm: 0.124270\n",
      "W1 norm: 0.221674\n",
      "iteration 1030 / 1225: loss 1.320006\n",
      "W2 norm: 0.125556\n",
      "W1 norm: 0.224207\n",
      "iteration 1040 / 1225: loss 1.434627\n",
      "W2 norm: 0.126666\n",
      "W1 norm: 0.226809\n",
      "iteration 1050 / 1225: loss 1.432653\n",
      "W2 norm: 0.127298\n",
      "W1 norm: 0.228830\n",
      "iteration 1060 / 1225: loss 1.402653\n",
      "W2 norm: 0.128700\n",
      "W1 norm: 0.231600\n",
      "iteration 1070 / 1225: loss 1.474285\n",
      "W2 norm: 0.129945\n",
      "W1 norm: 0.234114\n",
      "iteration 1080 / 1225: loss 1.366318\n",
      "W2 norm: 0.131132\n",
      "W1 norm: 0.236733\n",
      "iteration 1090 / 1225: loss 1.361503\n",
      "W2 norm: 0.131598\n",
      "W1 norm: 0.238775\n",
      "iteration 1100 / 1225: loss 1.368993\n",
      "W2 norm: 0.131430\n",
      "W1 norm: 0.240414\n",
      "iteration 1110 / 1225: loss 1.339835\n",
      "W2 norm: 0.132806\n",
      "W1 norm: 0.243149\n",
      "iteration 1120 / 1225: loss 1.351428\n",
      "W2 norm: 0.133286\n",
      "W1 norm: 0.245271\n",
      "iteration 1130 / 1225: loss 1.331627\n",
      "W2 norm: 0.135228\n",
      "W1 norm: 0.248761\n",
      "iteration 1140 / 1225: loss 1.520677\n",
      "W2 norm: 0.135417\n",
      "W1 norm: 0.250714\n",
      "iteration 1150 / 1225: loss 1.443765\n",
      "W2 norm: 0.136558\n",
      "W1 norm: 0.253569\n",
      "iteration 1160 / 1225: loss 1.485781\n",
      "W2 norm: 0.137305\n",
      "W1 norm: 0.256224\n",
      "iteration 1170 / 1225: loss 1.463751\n",
      "W2 norm: 0.137801\n",
      "W1 norm: 0.258217\n",
      "iteration 1180 / 1225: loss 1.314715\n",
      "W2 norm: 0.138637\n",
      "W1 norm: 0.260732\n",
      "iteration 1190 / 1225: loss 1.235901\n",
      "W2 norm: 0.139700\n",
      "W1 norm: 0.263358\n",
      "iteration 1200 / 1225: loss 1.382403\n",
      "W2 norm: 0.140539\n",
      "W1 norm: 0.266026\n",
      "iteration 1210 / 1225: loss 1.340345\n",
      "W2 norm: 0.141510\n",
      "W1 norm: 0.268477\n",
      "iteration 1220 / 1225: loss 1.328007\n",
      "W2 norm: 0.142103\n",
      "W1 norm: 0.270715\n",
      "number of epochs completed: 5\n",
      "Hyperparameter combinations completed: 1 / 1\n",
      "Best training accuracy achieved: 0.536531\n",
      "Best validation accuracy achieved: 0.507000\n",
      "\n",
      "Hyperparamters of the best net:\n",
      "hidden size: 204\n",
      "learning rate: 0.001000\n",
      "number of epochs: 5\n",
      "regularisation strength: 10^(-5.920819)\n",
      "\n",
      "Paramters in W1 are:\n",
      "[[  8.21014810e-04   3.28247023e-04   5.31776230e-04 ...,  -6.74569372e-05\n",
      "    9.68155235e-05   5.34810021e-04]\n",
      " [  1.79948614e-04   1.17577067e-04   3.67458953e-05 ...,  -3.75665878e-04\n",
      "    7.27921019e-04  -9.10018539e-06]\n",
      " [ -2.50363068e-06   2.12512659e-04  -1.88949294e-04 ...,  -2.37272802e-03\n",
      "    7.09418929e-04  -7.26147214e-04]\n",
      " ..., \n",
      " [ -2.11989957e-03   4.50423032e-04  -1.90864794e-04 ...,   1.31444459e-04\n",
      "    2.10967684e-03   6.52750493e-05]\n",
      " [  7.05481580e-04   3.49507693e-04  -4.65382018e-04 ...,   3.95306523e-04\n",
      "    1.57719058e-03  -1.77410068e-04]\n",
      " [  2.64105249e-03   5.72747974e-04  -6.01673888e-04 ...,  -5.66496714e-04\n",
      "   -1.19146611e-04  -5.95546914e-04]]\n",
      "Paramters in W2 are:\n",
      "[[ -4.59702445e-03   6.42653650e-03  -6.39635272e-03 ...,  -9.61179487e-03\n",
      "    2.49707905e-02  -3.77178098e-03]\n",
      " [ -4.79934163e-03  -9.53044605e-03  -2.48564608e-03 ...,   4.00774302e-03\n",
      "   -6.30048314e-03   7.38742309e-04]\n",
      " [  6.25220420e-03  -6.82143973e-03   8.59063253e-03 ...,  -1.11314031e-02\n",
      "    2.83651108e-03  -7.34349241e-03]\n",
      " ..., \n",
      " [ -6.23190368e-03  -1.40529977e-02   1.29687859e-02 ...,   3.84891493e-05\n",
      "   -1.00860405e-02  -1.60440436e-02]\n",
      " [ -4.10405169e-03  -2.14990257e-02   1.91097181e-02 ...,   1.88378889e-02\n",
      "   -1.24152492e-02  -2.24849358e-03]\n",
      " [  3.13343629e-03   7.87348353e-03   4.02216298e-03 ...,   9.99116031e-04\n",
      "   -7.42225682e-03  -5.91140512e-03]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temp teseting: 2017-08-23 22:15\n",
    "# when learning rate at 10 ** -2 and higher, overflow happens.\n",
    "# let's find out what the weight matrix W looks like for learning_rate = 10**-1\n",
    "learning_rate_range = [10 ** (-3)]\n",
    "\n",
    "hidden_size_range = [204]\n",
    "num_epochs_range = [5]\n",
    "reg_range = [1.20e-06]\n",
    "\n",
    "\n",
    "# pipe ranges into a list\n",
    "hyper_params_range = [hidden_size_range, \n",
    "                      learning_rate_range, \n",
    "                      num_epochs_range, \n",
    "                      reg_range]\n",
    "\n",
    "hyper_params_list = hyper_params_comb(hyper_params_range)\n",
    "tic = time.time()\n",
    "best_net, results, best_val_acc = net_tunning(X_train, y_train, \n",
    "                               X_val, y_val, \n",
    "                               hyper_params_list, verbose=True)\n",
    "toc = time.time()\n",
    "\n",
    "print('Paramters in W1 are:')\n",
    "print(best_net.params['W1'])\n",
    "print('Paramters in W2 are:')\n",
    "print(best_net.params['W2'])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3. learning rate tunning\n",
    "###### setting hyperparameters ###### \n",
    "# tunning parameter:\n",
    "# learning_rate_range = [1e-3]\n",
    "learning_rate_range = 10 ** np.linspace(-6, 1, 20)  # 1st coarse tunning\n",
    "\n",
    "# locked paramters:\n",
    "hidden_size_range = [204]\n",
    "num_epochs_range = [5]\n",
    "reg_range = [1.20e-06]\n",
    "\n",
    "# pipe ranges into a list\n",
    "hyper_params_range = [hidden_size_range, \n",
    "                      learning_rate_range, \n",
    "                      num_epochs_range, \n",
    "                      reg_range]\n",
    "\n",
    "hyper_params_list = hyper_params_comb(hyper_params_range)\n",
    "tic = time.time()\n",
    "best_net, results, best_val_acc = net_tunning(X_train, y_train, \n",
    "                               X_val, y_val, \n",
    "                               hyper_params_list, verbose=True)\n",
    "toc = time.time()\n",
    "\n",
    "\n",
    "####### time consumed ######\n",
    "print()\n",
    "print('Number of hyperparams to tune: %d' % len(hyper_params_list))\n",
    "print('Total time used: %f (seconds)' % (toc-tic))\n",
    "print('Total time per hyperparam: %f (seconds)' % (float(toc-tic)/float(len(hyper_params_list))))\n",
    "print()\n",
    "\n",
    "\n",
    "###### Visualisation of results ######\n",
    "# visualise: [learning rate] vs [train/val accuracy]\n",
    "reg_axis = [hyper_params[1] for hyper_params in sorted(results)]\n",
    "train_acc_history = [results[hyper_params][0] for hyper_params in sorted(results)]\n",
    "val_acc_history = [results[hyper_params][1] for hyper_params in sorted(results)]\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(reg_axis, val_acc_history, label='val')\n",
    "plt.plot(reg_axis, train_acc_history, label='train')\n",
    "plt.title('Tunning of learning rate')\n",
    "plt.legend()\n",
    "             \n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.log10(reg_axis), val_acc_history, label='val')\n",
    "plt.plot(np.log10(reg_axis), train_acc_history, label='train')\n",
    "plt.title('Tunning of regularisation strength, logirithm axis')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##### send a messgae to wechat #####\n",
    "itchat.send('Learning rate tunning completed! \\n '\n",
    "            '\\nTime used: \\n%f (secs)\\nBest validation accuracy: \\n%f' % (toc-tic, best_val_acc),\n",
    "            toUserName='filehelper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. Number of epochs tunning\n",
    "\n",
    "###### setting hyperparameters ###### \n",
    "# tunning parameter:\n",
    "# learning_rate_range = [1e-3]\n",
    "# num_epochs_range = np.arange(5, 15, 1)  # 1st coarse tunning, result: best 0.54 @ 14 epochs\n",
    "num_epochs_range = np.arange(14, 21, 1) # 2nd coarse tunning, rsult: best 0.54 @ 17 epochs\n",
    "\n",
    "# locked paramters:\n",
    "hidden_size_range = [204]\n",
    "reg_range = [1.20e-06]\n",
    "learning_rate_range = [1e-3]\n",
    "\n",
    "# pipe ranges into a list\n",
    "hyper_params_range = [hidden_size_range, \n",
    "                      learning_rate_range, \n",
    "                      num_epochs_range, \n",
    "                      reg_range]\n",
    "\n",
    "hyper_params_list = hyper_params_comb(hyper_params_range)\n",
    "tic = time.time()\n",
    "best_net, results, best_val_acc = net_tunning(X_train, y_train, \n",
    "                               X_val, y_val, \n",
    "                               hyper_params_list, verbose=False)\n",
    "toc = time.time()\n",
    "\n",
    "\n",
    "####### time consumed ######\n",
    "print()\n",
    "print('Number of hyperparams to tune: %d' % len(hyper_params_list))\n",
    "print('Total time used: %f (seconds)' % (toc-tic))\n",
    "print('Total time per hyperparam: %f (seconds)' % (float(toc-tic)/float(len(hyper_params_list))))\n",
    "print()\n",
    "\n",
    "\n",
    "###### Visualisation of results ######\n",
    "# visualise: [learning rate] vs [train/val accuracy]\n",
    "num_ep_axis = [hyper_params[2] for hyper_params in sorted(results)]\n",
    "train_acc_history = [results[hyper_params][0] for hyper_params in sorted(results)]\n",
    "val_acc_history = [results[hyper_params][1] for hyper_params in sorted(results)]\n",
    "\n",
    "plt.plot(num_ep_axis, val_acc_history, label='val')\n",
    "plt.plot(num_ep_axis, train_acc_history, label='train')\n",
    "plt.title('Tunning of number of epochs')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "##### send a messgae to wechat #####\n",
    "itchat.send('Number of epochs tunning completed!'\n",
    "            '\\nTime used: \\n%f (secs)\\nBest validation accuracy: \\n%f'\n",
    "            '\\nBest number of epochs: %d' \n",
    "            % (toc-tic, best_val_acc, best_net.hyper_params['num_epochs']),\n",
    "            toUserName='filehelper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combined fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
